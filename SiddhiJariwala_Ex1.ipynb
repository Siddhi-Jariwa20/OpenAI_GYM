{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d76fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_Learning Algorithm Approach\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "env_v = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "env_v.reset()\n",
    "print(\"The State Space: \", env_v.observation_space)\n",
    "print(\"The Action Space: \", env_v.action_space)\n",
    "l_value = env_v.observation_space.low\n",
    "h_value = env_v.observation_space.high\n",
    "print(\"The Bound Value of  X Axis: \", l_value[0], h_value[0])\n",
    "print(\"The Bound Value of  Y Axis: \", l_value[1], h_value[1])\n",
    "# Define Query-learning function\n",
    "def Query_Learning(env_v, learn_value, val_of_disc, eps_val, min_no_of_eps, no_of_episodes):\n",
    "    # Determine size of discretized state space\n",
    "    no_of_state = (env_v.observation_space.high - env_v.observation_space.low) * \\\n",
    "                 np.array([10, 100])\n",
    "    no_of_state = np.round(no_of_state, 0).astype(int) + 1\n",
    "    # Initialize Query table\n",
    "    Query_Table = np.random.uniform(low=-1, high=1,\n",
    "                          size=(no_of_state[0], no_of_state[1],\n",
    "                                env_v.action_space.n))\n",
    "    # Initializing the variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    # Calculating episodic reduction in eps_val\n",
    "    reduction = (eps_val - min_no_of_eps) / no_of_episodes\n",
    "    # Run Query learn_value algorithm\n",
    "    for i in range(no_of_episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        total_no_of_rewards, reward = 0, 0\n",
    "        state = env_v.reset()\n",
    "        adj_state_val = (state[0]- env_v.observation_space.low) * np.array([10, 100])\n",
    "        adj_state_val = np.round(adj_state_val, 0).astype(int)\n",
    "        while done != True:\n",
    "            # Rendering environment ( for last 5 no_of_episodes )\n",
    "            if i >= (no_of_episodes - 10):\n",
    "                env_v.render()\n",
    "            # Determining the next act_val\n",
    "            if np.random.random() < 1 - eps_val:\n",
    "                act_val = np.argmax(Query_Table[adj_state_val[0], adj_state_val[1]])\n",
    "            else:\n",
    "                act_val = np.random.randint(0, env_v.action_space.n)\n",
    "            # Getting to the next state and reward\n",
    "            next_state, reward, done, info,v = env_v.step(act_val)\n",
    "            adj_next_state = (next_state - env_v.observation_space.low) * np.array([10, 100])\n",
    "            adj_next_state = np.round(adj_next_state, 0).astype(int)\n",
    "            # Allowing for terminal states\n",
    "            if done and next_state[0] >= 0.5:\n",
    "                Query_Table[adj_state_val[0], adj_state_val[1], act_val] = reward\n",
    "            # Adjust Query value for current state\n",
    "            else:\n",
    "                delta = learn_value * (reward +\n",
    "                                    val_of_disc * np.max(Query_Table[adj_next_state[0],\n",
    "                                                        adj_next_state[1]]) -\n",
    "                                    Query_Table[adj_state_val[0], adj_state_val[1], act_val])\n",
    "                Query_Table[adj_state_val[0], adj_state_val[1], act_val] += delta\n",
    "            total_no_of_rewards += reward\n",
    "            adj_state_val = adj_next_state\n",
    "        if eps_val > min_no_of_eps:\n",
    "            eps_val -= reduction\n",
    "        # Tracking of the  rewards\n",
    "        reward_list.append(total_no_of_rewards)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            reward_val_av = np.mean(reward_list)\n",
    "            ave_reward_list.append(reward_val_av)\n",
    "            reward_list = []\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Episode {} Average Reward: {}'.format(i + 1, reward_val_av))\n",
    "    env_v.close()\n",
    "    return ave_reward_list\n",
    "# Executing Query-learn_value algorithm\n",
    "rewards = Query_Learning(env_v, 0.2, 0.9, 0.8, 0, 5000)\n",
    "# Plotting the  Rewards\n",
    "plt.plot(100 * (np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('The Episodes')\n",
    "plt.ylabel('The Average Reward Value')\n",
    "plt.title('The Average Reward vs The Episodes')\n",
    "plt.savefig('The rewards image.jpg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Approach\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "#printing the value of observation and action space\n",
    "print(\"The State Space: \", env.observation_space)\n",
    "print(\"The Action Space: \", env.action_space)\n",
    "\n",
    "#getting the high and low value\n",
    "l_value = env.observation_space.low\n",
    "h_value = env.observation_space.high\n",
    "\n",
    "#printing the Bounds for X and Y axis respectively\n",
    "print(\"The Bound Value for  X Axis: \", l_value[0], h_value[0])\n",
    "print(\"The Bound Value for Y Axis: \", l_value[1], h_value[1])\n",
    "#resetting the environment\n",
    "env.reset()\n",
    "# A loop for taking a random action for 60000 times\n",
    "for _ in range(6000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()\n",
    "# Executing Query-learning algorithm\n",
    "tot_rewards = Query_Learning(env, 0.2, 0.9, 0.8, 0, 6000)\n",
    "\n",
    "# Plotting the rewards\n",
    "plt.plot(100 * (np.arange(len(tot_rewards)) + 1), tot_rewards)\n",
    "plt.xlabel('Events')\n",
    "plt.ylabel('The Value of Average Reward')\n",
    "plt.title('The Average Reward vs Events')\n",
    "plt.savefig('rewards_image.jpg')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
